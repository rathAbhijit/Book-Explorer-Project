import requests
import os
import time
from urllib.parse import quote
from math import ceil
import google.generativeai as genai
from openai import OpenAI
from django.conf import settings
from django.core.cache import cache
from django.db.models import Avg

from .models import Review, Book, UserBookInteraction
from .serializers import (
    BookDetailSerializer,
    ReviewMiniSerializer,
    UserBookInteractionMiniSerializer,
)
import io
from typing import List
from pdfminer.high_level import extract_text as pdf_extract_text
from docx import Document as DocxDocument
# --- Configure Gemini ---
try:
    genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
except Exception:
    print("âš ï¸ Warning: GEMINI_API_KEY not found or invalid. AI features will fail.")

# -------------------------------
# External API helpers
# -------------------------------

def search_google_books(query, max_results=20):
    """Query the Google Books API with a search term or subject safely."""
    url = "https://www.googleapis.com/books/v1/volumes"
    max_results = min(max_results, 40)  # Google API limit
    safe_query = quote(query)

    params = {
        "q": safe_query,
        "maxResults": max_results,
        "key": getattr(settings, "GOOGLE_BOOKS_API_KEY", None),
    }
    try:
        response = requests.get(url, params=params, timeout=10)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.HTTPError as e:
        status_code = e.response.status_code if e.response else None
        if status_code == 401:
            return {
                "error": "Google Books API returned 401 Unauthorized. Invalid or expired API key.",
                "hint": "Check your GOOGLE_BOOKS_API_KEY in settings or .env file."
            }
        elif status_code == 403:
            return {
                "error": "Google Books API quota exceeded or permission denied.",
                "hint": "Try again later or enable the Books API in your Google Cloud project."
            }
        else:
            return {
                "error": f"Google Books API Error: {str(e)}",
                "hint": "See backend logs for more details."
            }
    except requests.RequestException as e:
        return {
            "error": f"Network or timeout error: {str(e)}",
            "hint": "Ensure your server can reach the Google Books API."
        }
    

# RENAMED and FIXED: This now correctly fetches raw API data.
def fetch_google_book_by_id(google_id):
    """Get details for a specific book by Google ID from the API."""
    url = f"https://www.googleapis.com/books/v1/volumes/{google_id}"
    params = {"key": getattr(settings, "GOOGLE_BOOKS_API_KEY", None)}
    try:
        response = requests.get(url, params=params, timeout=10)
        response.raise_for_status()
        return response.json()
    except requests.RequestException as e:
        print(f"Google Books API Error fetching ID {google_id}: {e}")
        return None


def get_nyt_bestsellers():
    """Fetch bestseller list from NYT API if valid key is available."""
    api_key = getattr(settings, "NYT_API_KEY", None)
    if not api_key:
        print("âš ï¸ NYT_API_KEY not set â€” skipping NYT bestseller fetch.")
        return []

    url = "https://api.nytimes.com/svc/books/v3/lists/current/hardcover-fiction.json"
    params = {"api-key": api_key}
    try:
        r = requests.get(url, params=params, timeout=10)
        r.raise_for_status()
        data = r.json().get("results", {}).get("books", [])
        return [
            {
                "title": b.get("title"),
                "authors": [b.get("author")] if b.get("author") else [],
                "thumbnail": b.get("book_image"),
                "description": b.get("description"),
                "rank": b.get("rank"),
                "amazon_url": b.get("amazon_product_url"),
            }
            for b in data
        ]
    except requests.exceptions.HTTPError as e:
        print(f"NYT API Error: {e}")
        return []
    except Exception as e:
        print(f"NYT Bestseller Fetch Error: {e}")
        return []
    


# -------------------------------
# Normalizers (Google + NYT)
# -------------------------------

def normalize_google_book(item):
    """Normalize Google Books API item into unified schema."""
    volume = item.get("volumeInfo", {})
    return {
        "google_id": item.get("id"),
        "title": volume.get("title", "Unknown Title"),
        "authors": volume.get("authors", []),
        "published_date": volume.get("publishedDate"),
        "categories": volume.get("categories", []),
        "thumbnail": (volume.get("imageLinks", {}) or {}).get("thumbnail"),
        "description": volume.get("description"),
        "average_rating": volume.get("averageRating"),
    }

def normalize_nyt_book(item):
    """Normalize NYT bestseller book into unified schema."""
    # ... (rest of function is fine)
    return {
        "google_id": None,
        "title": item.get("title"),
        "authors": [item.get("author")] if item.get("author") else [],
        "thumbnail": item.get("book_image"),
        "description": item.get("description"),
        "amazon_url": item.get("amazon_product_url"),
        "rank": item.get("rank"),
    }

# -------------------------------
# DB caching / get_or_create
# -------------------------------

# IMPROVED: This now saves a more complete record to your database.
def get_or_create_book_details(google_id):
    """
    Check DB for book; if missing, fetch from Google, normalize,
    and save a complete record.
    """
    try:
        return Book.objects.get(google_id=google_id)
    except Book.DoesNotExist:
        data = fetch_google_book_by_id(google_id)
        if not data:
            return None

        normalized_data = normalize_google_book(data)
        
        book, created = Book.objects.update_or_create(
            google_id=google_id,
            defaults={
                "title": normalized_data.get("title", "Unknown Title"),
                "authors": normalized_data.get("authors", []),
                "published_date": normalized_data.get("published_date"),
                "thumbnail_url": normalized_data.get("thumbnail"),
                "short_description": normalized_data.get("description"),
            }
        )
        return book

# -------------------------------
# High-level business logic (with caching)
# -------------------------------

# ADDED: Caching for performance
def get_genre_top_books(limit=10):
    """Get top book from each genre (Google Books), with caching."""
    cache_key = "genre_top_books"
    cached_books = cache.get(cache_key)
    if cached_books:
        return cached_books

    genres = ["Science Fiction", "Science", "History", "Biography", "Fantasy", "Romance"]
    books = []
    for genre in genres[:limit]:
        data = search_google_books(f"subject:{genre}", max_results=1)
        if data and "items" in data:
            books.append(normalize_google_book(data["items"][0]))
    
    cache.set(cache_key, books, 60 * 60 * 6)  # Cache for 6 hours
    return books

# ADDED: Caching for performance
def get_recent_books(limit=10):
    """Get recently published books (Google Books), with caching."""
    cache_key = "recent_books"
    cached_books = cache.get(cache_key)
    if cached_books:
        return cached_books
    
    data = search_google_books("newest", max_results=limit)
    if not data:
        return []
    books = [normalize_google_book(item) for item in data.get("items", [])]
    cache.set(cache_key, books, 60 * 60 * 6)  # Cache for 6 hours
    return books

# ADDED: Caching for performance
def get_bestsellers(limit=10):
    """
    Unified bestseller fetcher:
    1ï¸âƒ£ Try NYT API
    2ï¸âƒ£ Fallback to Google Books â€œbestseller OR popular booksâ€
    3ï¸âƒ£ Fallback to local DB (top-rated)
    """
    cache_key = "bestsellers_combined"
    cached = cache.get(cache_key)
    if cached:
        return cached[:limit]

    books = []

    # 1ï¸âƒ£ Try NYT API
    nyt_books = get_nyt_bestsellers()
    if nyt_books:
        books = nyt_books[:limit]
        print("âœ… Using NYT bestsellers")
    else:
        # 2ï¸âƒ£ Google Books Fallback
        print("âš ï¸ Falling back to Google Books bestseller search...")
        google_data = search_google_books("bestseller OR popular books", max_results=limit)
        if google_data and "items" in google_data:
            books = [
                {
                    "google_id": i["id"],
                    "title": i["volumeInfo"].get("title"),
                    "authors": i["volumeInfo"].get("authors", []),
                    "thumbnail": (i["volumeInfo"].get("imageLinks", {}) or {}).get("thumbnail"),
                    "description": i["volumeInfo"].get("description"),
                    "average_rating": i["volumeInfo"].get("averageRating"),
                }
                for i in google_data["items"]
            ]
            print("âœ… Using Google Books fallback.")
        else:
            # 3ï¸âƒ£ Local DB Fallback
            print("âš ï¸ Falling back to local DB top-rated books.")
            local_books = (
                Book.objects.filter(average_rating__isnull=False)
                .order_by("-average_rating")[:limit]
                .values("google_id", "title", "authors", "thumbnail_url", "average_rating")
            )
            books = list(local_books)

    cache.set(cache_key, books, timeout=60 * 60 * 6)
    return books

# -------------------------------
# AI Summary (Gemini / caching)
# -------------------------------
def generate_and_cache_ai_summary(book_id: str):
    """
    Generate a spoiler-free AI summary using Gemini 2.5 Flash.
    Falls back to OpenAI GPT-4 Turbo if Gemini fails.
    """

    cache_key = f"book_summary_gemini_{book_id}"
    summary = cache.get(cache_key)
    if summary:
        print(f"âœ… Cache hit for {book_id}")
        return summary

    try:
        book = Book.objects.get(google_id=book_id)
    except Book.DoesNotExist:
        return "Summary not available because the book is not in our database."

    # --- Prompt ---
    prompt = (
        f"Write a spoiler-free, engaging, and concise summary (around 150 words) "
        f"for the book titled '{book.title}' by {', '.join(book.authors or ['Unknown Author'])}. "
        f"Focus on the tone, main ideas, and emotional appeal â€” avoid revealing any major plot twists."
    )

    summary = None
    error_message = None

    # --- Try Gemini first ---
    try:
        print(f"âš™ï¸ Using Gemini for '{book.title}'...")
        genai.configure(api_key=settings.GEMINI_API_KEY)
        model = genai.GenerativeModel("gemini-2.5-flash")

        response = model.generate_content(prompt)
        if response and getattr(response, "text", None):
            summary = response.text.strip()
            print(f"âœ… Gemini succeeded for '{book.title}'")
        else:
            raise ValueError("Gemini returned empty response")

    except Exception as e:
        error_message = str(e)
        print(f"âš ï¸ Gemini failed: {error_message}")

    # --- Fallback: OpenAI GPT-4 Turbo ---
    if not summary:
        try:
            print(f"ðŸ¤– Falling back to OpenAI for '{book.title}'...")
            client = OpenAI(api_key=settings.OPENAI_API_KEY)
            completion = client.chat.completions.create(
                model="gpt-4-turbo",
                messages=[
                    {"role": "system", "content": "You are an expert book summarizer."},
                    {"role": "user", "content": prompt},
                ],
                max_tokens=250,
            )
            summary = completion.choices[0].message.content.strip()
            print(f"âœ… OpenAI succeeded for '{book.title}'")
        except Exception as e:
            print(f"ðŸ”¥ OpenAI fallback also failed: {e}")
            summary = (
                f"'{book.title}' by {', '.join(book.authors or ['Unknown Author'])} "
                "is a remarkable book that explores deep ideas and emotions. "
                "The detailed AI summary is currently unavailable."
            )

    # --- Cache and persist ---
    cache.set(cache_key, summary, 60 * 60 * 24)  # Cache for 24h
    try:
        if not book.ai_summary or book.ai_summary != summary:
            book.ai_summary = summary
            book.save(update_fields=["ai_summary"])
            print(f"ðŸ’¾ Saved AI summary for '{book.title}'")
    except Exception as e:
        print(f"âš ï¸ DB save error: {e}")

    return summary

# ============================================================
# ðŸ”¹ AUTHOR SERVICES
# ============================================================
def fetch_author_details(author_name: str):
    """
    Try fetching author details from Open Library API.
    Falls back to AI-generated bio if not found.
    """
    from django.conf import settings
    from django.core.cache import cache

    cache_key = f"author_detail_{author_name}"
    cached = cache.get(cache_key)
    if cached:
        return cached

    # --- Try Open Library API ---
    try:
        response = requests.get(
            f"https://openlibrary.org/search/authors.json?q={author_name}", timeout=10
        )
        response.raise_for_status()
        data = response.json()

        if not data.get("docs"):
            raise ValueError("No author found in Open Library")

        doc = data["docs"][0]
        author_info = {
            "name": doc.get("name", author_name),
            "birth_date": doc.get("birth_date"),
            "death_date": doc.get("death_date"),
            "top_work": doc.get("top_work"),
            "work_count": doc.get("work_count"),
            "top_subjects": doc.get("top_subjects", [])[:5],
            "bio": doc.get("bio") if isinstance(doc.get("bio"), str) else None,
            "active_years": f"{doc.get('birth_date', '?')} â€“ {doc.get('death_date', '?')}",
        }

    except Exception:
        # --- Fallback to OpenAI GPT bio ---
        client = OpenAI(api_key=settings.OPENAI_API_KEY)
        prompt = (
            f"Write a short, factual biography (under 120 words) of the author '{author_name}'. "
            "Include their writing style, themes, and literary significance if known. "
            "Avoid making up data if unknown."
        )
        try:
            completion = client.chat.completions.create(
                model="gpt-4-turbo",
                messages=[
                    {"role": "system", "content": "You are a literary historian."},
                    {"role": "user", "content": prompt},
                ],
            )
            bio = completion.choices[0].message.content.strip()
        except Exception:
            bio = "Biography unavailable at the moment."

        author_info = {
            "name": author_name,
            "bio": bio,
            "top_subjects": [],
            "active_years": None,
            "top_work": None,
            "work_count": None,
        }

    cache.set(cache_key, author_info, timeout=60 * 60 * 12)
    return author_info


def paginate_list(items, page=1, page_size=10):
    """Simple pagination utility for list-based data."""
    total = len(items)
    total_pages = ceil(total / page_size)
    start = (page - 1) * page_size
    end = start + page_size
    paginated = items[start:end]

    return {
        "page": page,
        "page_size": page_size,
        "total_items": total,
        "total_pages": total_pages,
        "results": paginated,
    }

# ============================================================
# ðŸ”¹ Explore Page Handler (Unified Logic)
# ============================================================
def get_explore_books(query=None, genre=None, sort=None, limit=50, page=1, page_size=10):
    """
    Unified backend handler for the Explore page (search, genre, sorting) with pagination.
    Default mode returns curated genre-based sections.
    """
    cache_key = f"explore_{query or genre or sort or 'default'}_{limit}"
    cached = cache.get(cache_key)
    if cached:
        return cached

    # ==============================
    # 1ï¸âƒ£ Search Mode
    # ==============================
    if query:
        data = search_google_books(query, max_results=limit)
        books = [normalize_google_book(item) for item in data.get("items", [])] if data and "items" in data else []
        paginated = paginate_list(books, page=page, page_size=page_size)
        result = {"mode": "search", "data": paginated}
        cache.set(cache_key, result, 60 * 60 * 3)
        return result

    # ==============================
    # 2ï¸âƒ£ Genre Mode
    # ==============================
    if genre:
        data = search_google_books(f"subject:{genre}", max_results=limit)
        books = [normalize_google_book(item) for item in data.get("items", [])] if data and "items" in data else []
        paginated = paginate_list(books, page=page, page_size=page_size)
        result = {"mode": "genre", "data": paginated}
        cache.set(cache_key, result, 60 * 60 * 3)
        return result

    # ==============================
    # 3ï¸âƒ£ Sorting Mode
    # ==============================
    if sort == "newest":
        books = get_recent_books(limit=limit)
        result = {"mode": "sorted", "data": paginate_list(books, page=page, page_size=page_size)}
        cache.set(cache_key, result, 60 * 60 * 3)
        return result
    elif sort == "bestsellers":
        books = get_bestsellers(limit=limit)
        result = {"mode": "sorted", "data": paginate_list(books, page=page, page_size=page_size)}
        cache.set(cache_key, result, 60 * 60 * 3)
        return result

    # ==============================
    # 4ï¸âƒ£ Default Mode â€” Curated Explore View
    # ==============================
    print("âœ¨ Building curated Explore default view...")

    sections = {}
    genres = ["Fiction", "Novel", "Mystery", "History", "Science", "Science Fiction"]

    # Each genre â†’ 5 books
    for g in genres:
        data = search_google_books(f"subject:{g}", max_results=5)
        if data and "items" in data:
            sections[g.lower().replace(" ", "_")] = [normalize_google_book(item) for item in data["items"]]

    # Recent and popular sections
    sections["recent"] = get_recent_books(limit=8)
    sections["popular"] = get_bestsellers(limit=8)

    result = {
        "mode": "default",
        "sections": sections
    }

    cache.set(cache_key, result, 60 * 60 * 3)
    return result




# ============================================================
# ðŸ”¹ BOOK DETAIL (FULL) SERVICE
# ============================================================

def get_full_book_details(book_id, user=None):
    """
    Return complete book details with average rating, reviews,
    and user-specific interaction status.
    Cached for 30 minutes for performance.
    """
    cache_key = f"book_full_detail_{book_id}_{user.id if user else 'anon'}"
    cached = cache.get(cache_key)
    if cached:
        return cached

    try:
        book = Book.objects.get(google_id=book_id)
    except Book.DoesNotExist:
        return {"error": "Book not found."}

    # Base book data
    book_data = BookDetailSerializer(book).data

    # Reviews and average rating
    reviews = Review.objects.filter(book=book).order_by("-created_at")
    review_data = ReviewMiniSerializer(reviews, many=True).data
    avg_rating = reviews.aggregate(avg=Avg("rating"))["avg"]
    book_data["average_rating"] = avg_rating or book.average_rating

    # User interaction (if logged in)
    user_interaction_data = None
    if user and user.is_authenticated:
        interaction = UserBookInteraction.objects.filter(user=user, book=book).first()
        if interaction:
            user_interaction_data = UserBookInteractionMiniSerializer(interaction).data

    result = {
        "book": book_data,
        "reviews": review_data,
        "average_rating": avg_rating or 0,
        "user_interaction": user_interaction_data,
    }

    # Cache for 30 minutes
    cache.set(cache_key, result, timeout=60 * 30)
    return result

# ============================================================
# ðŸ”¹ CACHE INVALIDATION HELPERS
# ============================================================

def clear_book_detail_cache(book_id, user_id=None):
    """
    Clears all cached versions of book detail (both user-specific and anonymous).
    """
    cache.delete(f"book_full_detail_{book_id}_anon")
    if user_id:
        cache.delete(f"book_full_detail_{book_id}_{user_id}")

# books/services.py (append near bottom, before cache helpers if you want)

import io
from typing import List
from pdfminer.high_level import extract_text as pdf_extract_text
from docx import Document as DocxDocument
from django.core.cache import cache

# ========== HuggingFace Summarizer (singleton) ==========
_SUMMARIZER = None

def _get_summarizer():
    """
    Lazy-load a local HF summarization pipeline.
    Default: distilbart for speed. Switch to bart-large-cnn if you prefer quality.
    """
    global _SUMMARIZER
    if _SUMMARIZER is not None:
        return _SUMMARIZER

    from transformers import pipeline

    # Light model, good on CPU:
    model_name = "sshleifer/distilbart-cnn-12-6"
    # Heavier but higher quality:
    # model_name = "facebook/bart-large-cnn"

    _SUMMARIZER = pipeline(
        "summarization",
        model=model_name,
        tokenizer=model_name,
        framework="pt",            # Uses torch
        device=-1                  # CPU; set to 0 for GPU
    )
    return _SUMMARIZER


# ========== Text Extraction helpers ==========
def extract_text_from_pdf(fp: io.BytesIO) -> str:
    try:
        return pdf_extract_text(fp)
    except Exception as e:
        raise ValueError(f"Failed to read PDF: {e}")

def extract_text_from_docx(fp: io.BytesIO) -> str:
    try:
        doc = DocxDocument(fp)
        return "\n".join([p.text for p in doc.paragraphs])
    except Exception as e:
        raise ValueError(f"Failed to read DOCX: {e}")

def extract_text_from_txt(fp: io.BytesIO) -> str:
    try:
        data = fp.read()
        try:
            return data.decode("utf-8")
        except UnicodeDecodeError:
            return data.decode("latin-1", errors="ignore")
    except Exception as e:
        raise ValueError(f"Failed to read TXT: {e}")


def extract_text_from_upload(django_file) -> str:
    content_type = django_file.content_type
    buf = io.BytesIO(django_file.read())

    if content_type == "application/pdf":
        return extract_text_from_pdf(buf)
    elif content_type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document":
        return extract_text_from_docx(buf)
    elif content_type == "text/plain":
        return extract_text_from_txt(buf)
    else:
        raise ValueError("Unsupported file type.")


# ========== Chunking + Two-pass summarization ==========
def _split_into_chunks(text: str, max_chars: int = 3000) -> List[str]:
    """
    Conservative chunking by characters. Keeps words intact.
    """
    text = text.strip()
    if not text:
        return []

    chunks = []
    start = 0
    n = len(text)
    while start < n:
        end = min(start + max_chars, n)
        # try to cut at nearest sentence end for better quality
        cut = end
        dot = text.rfind(".", start, end)
        qst = text.rfind("?", start, end)
        exc = text.rfind("!", start, end)
        sep = max(dot, qst, exc)
        if sep > start + max_chars * 0.6:
            cut = sep + 1
        chunk = text[start:cut].strip()
        if chunk:
            chunks.append(chunk)
        start = cut
    return chunks

def summarize_text_local(text: str, max_summary_words: int = 250) -> dict:
    """
    Local summarizer using Hugging Face transformer pipeline.
    Handles long inputs safely, with chunking and graceful fallback.
    """
    text = (text or "").strip()
    if not text:
        return {
            "summary": "",
            "input_words": 0,
            "summary_words": 0,
            "compression_ratio": 1.0,
            "chunks": 0,
            "error": "Empty input text."
        }

    summarizer = _get_summarizer()
    words = text.split()
    input_words = len(words)
    target_tokens = max_summary_words * 1.3

    # Hard upper bound on doc size (e.g. 25K chars â‰ˆ 3000â€“4000 tokens)
    if len(text) > 25000:
        return {
            "summary": None,
            "input_words": input_words,
            "summary_words": 0,
            "compression_ratio": 0,
            "chunks": 0,
            "error": "Document too long for summarization. Please upload a smaller file (<25K characters)."
        }

    # --- Safe chunk splitting
    chunks = _split_into_chunks(text, max_chars=1500)
    summaries = []

    for ch in chunks:
        try:
            res = summarizer(
                ch[:3000],  # truncate for token safety
                max_length=int(min(300, target_tokens)),
                min_length=int(max(50, target_tokens // 3)),
                do_sample=False,
            )
            summaries.append(res[0]["summary_text"].strip())
        except Exception as e:
            print(f"âš ï¸ Summarization failed on chunk: {e}")
            continue

    if not summaries:
        return {
            "summary": None,
            "input_words": input_words,
            "summary_words": 0,
            "compression_ratio": 0,
            "chunks": len(chunks),
            "error": "All text chunks failed to summarize. Try a smaller file."
        }

    # Combine summaries if multiple chunks
    combined = " ".join(summaries)
    if len(chunks) > 1 and len(combined.split()) > max_summary_words:
        try:
            res2 = summarizer(
                combined[:3000],
                max_length=int(min(400, target_tokens)),
                min_length=int(max(100, target_tokens // 2)),
                do_sample=False,
            )
            final_summary = res2[0]["summary_text"].strip()
        except Exception as e:
            print(f"âš ï¸ Second pass summarization failed: {e}")
            final_summary = combined
    else:
        final_summary = combined

    summary_words = len(final_summary.split())
    compression_ratio = max(1.0, (input_words / max(1, summary_words)))

    return {
        "summary": final_summary,
        "input_words": input_words,
        "summary_words": summary_words,
        "compression_ratio": round(compression_ratio, 2),
        "chunks": len(chunks),
        "error": None
    }


def summarize_user_upload(django_file, max_summary_words: int = 250):
    """
    Extracts text from uploaded file (PDF/DOCX/TXT) and generates summary safely.
    """
    try:
        text = extract_text_from_upload(django_file)
        text = (text or "").strip()
    except Exception as e:
        print(f"âš ï¸ Error extracting text: {e}")
        return {
            "summary": None,
            "error": "Could not read the uploaded document. Please check the file format or encoding."
        }

    if not text:
        return {
            "summary": None,
            "error": "The uploaded document is empty or unreadable."
        }

    # Optional pre-truncation to avoid model crashes
    if len(text) > 25000:
        text = text[:25000]
        truncated = True
    else:
        truncated = False

    result = summarize_text_local(text, max_summary_words=max_summary_words)

    # Append truncation note if needed
    if truncated and not result.get("error"):
        result["note"] = "âš ï¸ Document was truncated to fit model limits (25K characters)."

    return result


# ========== Public service entry points ==========
def summarize_user_text(text: str, max_summary_words: int = 250) -> dict:
    """
    Summarize raw text (no upload). Cached by hash to save time.
    """
    import hashlib
    key = f"user_summary_text_{hashlib.md5((text + str(max_summary_words)).encode('utf-8')).hexdigest()}"
    cached = cache.get(key)
    if cached:
        return {**cached, "cached": True}

    result = summarize_text_local(text, max_summary_words=max_summary_words)
    cache.set(key, result, 60 * 60 * 6)
    return result


